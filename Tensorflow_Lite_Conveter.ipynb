{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[TensorFlow Lite Converter] Tensorflow Lite Conveter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMdY5juE9hseAAhb0Lin30l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weilly0912/Core-ML/blob/main/Tensorflow_Lite_Conveter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkiOl08yRUJQ"
      },
      "source": [
        "# **TensorFlow Lite 模組轉換大解密**\n",
        "[@PINTO 大神 文章](https://qiita.com/PINTO/items/865250ee23a15339d556#4-2-1-4-weight-quantization-from-saved_model-weight-only-quantization)\n",
        "\n",
        "[@PINTO 大神 Source Code](https://github.com/PINTO0309/PINTO_model_zoo)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **0.Tensorflow Version 切換**"
      ],
      "metadata": {
        "id": "x7PLPUagoGMW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_V5__nH0ArO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8619514f-7f73-4268-df34-caf051c5b684"
      },
      "source": [
        "# 切換版本\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N-iE5vy7Ur6"
      },
      "source": [
        "## **1.Tensorflow 1.x / 2.x Model 應用方式**\n",
        "\n",
        "*   (1) *ckpt to TensorFlow Lite ( Tensorflow 1.x version )*\n",
        "\n",
        "*   (2) *freezen graph to TensorFlow Lite ( Tensorflow 1.x version )*\n",
        "\n",
        "*   (3) *Savemodel to TensorFlow Lite   ( Tensorflow 2.x version)*\n",
        "\n",
        "*   (4) *Example ( Tensorflow 1.x version )*\n",
        "\n",
        "       4.1 重新儲存 ckpt\n",
        "\n",
        "       4.2 ckpt 轉成 freezen graph\n",
        "\n",
        "       4.3 freezen graph 轉成 saved_model\n",
        "      \n",
        "       4.4 轉成 TensorFlow Lite\n",
        "\n",
        "*   (5) *ReBuild-Inupt Savemodel to TensorFlow Lite*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**(1) ckpt to TensorFlow Lite**"
      ],
      "metadata": {
        "id": "xHueRlmLm60Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow 1.x \n",
        "import tensorflow as tf\n",
        "from src import transform # 讀取模組架構, https://github.com/TurboCome/Style-transfer-of-picture/blob/master/src/transform.py\n",
        "g = tf.compat.v1.Graph()\n",
        "soft_config = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n",
        "soft_config.gpu_options.allow_growth = True\n",
        "with g.as_default(), tf.compat.v1.Session(config=soft_config) as sess:\n",
        "  img_placeholder = tf.compat.v1.placeholder(tf.float32, shape=[1, 474, 712, 3], name='img_placeholder') #輸入端節點(Netron 查看)\n",
        "  preds = transform.net(img_placeholder) # 輸出端節點 ( Netron 查看 )\n",
        "  saver = tf.compat.v1.train.Saver()\n",
        "  saver.restore(sess, \"model.ckpt\" ) # ckpt file\n",
        "  converter = tf.compat.v1.lite.TFLiteConverter.from_session(sess, [img_placeholder], [preds]) \n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  tflite_model = converter.convert()\n",
        "  with tf.io.gfile.GFile( \"model.tflite\", 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "  print(\"Integer Quantization complete! -model.tflite\")"
      ],
      "metadata": {
        "id": "p0gtuNWinyQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxcwBmgkfq-n"
      },
      "source": [
        "###**(2) freezen graph to TensorFlow Lite**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmDkKb5Efw5l"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "# Weight Quantization - Input/Output=float32\n",
        "graph_def_file=\"tflite_graph_with_postprocess.pb\"\n",
        "input_arrays=[\"normalized_input_image_tensor\"]\n",
        "output_arrays=['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1', \n",
        "               'TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']\n",
        "input_tensor={\"normalized_input_image_tensor\":[1,300,300,3]}\n",
        "converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays,output_arrays,input_tensor)\n",
        "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "converter.allow_custom_ops = True\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('./ssdlite_mobilenet_v2_voc_300_weight_quant.tflite', 'wb') as w:\n",
        "    w.write(tflite_quant_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**(3) Savemodel to TensorFlow Lite**"
      ],
      "metadata": {
        "id": "722T9tLAoyZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model( \"savedmodel\" ) \n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "with tf.io.gfile.GFile( \"model.tflite\" , 'wb') as f:\n",
        "   f.write(tflite_model)\n",
        "print(\"Quantization complete! - model.tflite\")"
      ],
      "metadata": {
        "id": "d0Ytsuapo3VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**(4) Example : 以範例 Cartoonize 介紹**"
      ],
      "metadata": {
        "id": "MQdZkxzHnJAK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reoJNoZ-uf75"
      },
      "source": [
        "# Dowload API\n",
        "%cd /root\n",
        "!git clone https://github.com/SystemErrorWang/White-box-Cartoonization\n",
        "\n",
        "# 須修改與執行檔案兩次，修改內容依下\n",
        "%cd /root/White-box-Cartoonization/test_code\n",
        "!python3 cartoonize.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cQDYDwK8Ztg"
      },
      "source": [
        "#需要更改 Cartoonize \n",
        "#須從代碼找到輸入端與輸出端資訊\n",
        "\n",
        "# 4.1 重新輸出儲存檔案 ckpt\n",
        "# checkpoint & **.index & **.meta\n",
        "def cartoonize(load_folder, save_folder, model_path):\n",
        "    import sys\n",
        "    import shutil\n",
        "    shutil.rmtree('./export', ignore_errors=True)\n",
        "    input_photo = tf.placeholder(tf.float32, [1, 720, 720, 3], name='input') #<-- here\n",
        "    network_out = network.unet_generator(input_photo)# 須加入架構\n",
        "    final_out = guided_filter.guided_filter(input_photo, network_out, r=1, eps=5e-3)\n",
        "    print(\"input_photo.name =\", input_photo.name)\n",
        "    print(\"input_photo.shape =\", input_photo.shape)\n",
        "    print(\"final_out.name =\", final_out.name)\n",
        "    print(\"final_out.shape =\", final_out.shape)\n",
        "    all_vars = tf.trainable_variables()\n",
        "    gene_vars = [var for var in all_vars if 'generator' in var.name]\n",
        "    saver = tf.train.Saver(var_list=gene_vars)\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    sess = tf.Session(config=config)\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver.restore(sess, tf.train.latest_checkpoint(model_path)) #/root/White-box-Cartoonization/test_code/saved_models\n",
        "    saver.save(sess, './export/model.ckpt') #<-- here\n",
        "    sys.exit(0) #<-- here & delete below\n",
        "\n",
        "# 4.2 重新轉換為 Freeze_Graph\n",
        "# **.pbtxt and  **.pb\n",
        "def cartoonize(load_folder, save_folder, model_path):\n",
        "    import sys\n",
        "    graph = tf.get_default_graph()\n",
        "    sess = tf.Session()\n",
        "    saver = tf.train.import_meta_graph('./export/model.ckpt.meta')\n",
        "    saver.restore(sess, './export/model.ckpt')\n",
        "    tf.train.write_graph(sess.graph_def, './export', 'white_box_cartoonization_freeze_graph.pbtxt', as_text=True)\n",
        "    tf.train.write_graph(sess.graph_def, './export', 'white_box_cartoonization_freeze_graph.pb', as_text=False)\n",
        "    sys.exit(0)\n",
        "\n",
        "# 4.3 Freeze_Graph to Saved_model\n",
        "# TF 1.x 適用\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import shutil\n",
        "from tensorflow.python import ops\n",
        "\n",
        "def get_graph_def_from_file(graph_filepath):\n",
        "  tf.compat.v1.reset_default_graph()\n",
        "  with ops.Graph().as_default():\n",
        "    with tf.compat.v1.gfile.GFile(graph_filepath, 'rb') as f:\n",
        "      graph_def = tf.compat.v1.GraphDef()\n",
        "      graph_def.ParseFromString(f.read())\n",
        "      return graph_def\n",
        "\n",
        "def convert_graph_def_to_saved_model(export_dir, graph_filepath, input_name, outputs):\n",
        "  graph_def = get_graph_def_from_file(graph_filepath)\n",
        "  with tf.compat.v1.Session(graph=tf.Graph()) as session:\n",
        "    tf.import_graph_def(graph_def, name='')\n",
        "    tf.compat.v1.saved_model.simple_save(\n",
        "        session,\n",
        "        export_dir,# change input_image to node.name if you know the name\n",
        "        inputs={input_name: session.graph.get_tensor_by_name('{}:0'.format(node.name))\n",
        "            for node in graph_def.node if node.op=='Placeholder'},\n",
        "        outputs={t.rstrip(\":0\"):session.graph.get_tensor_by_name(t) for t in outputs}\n",
        "    )\n",
        "    print('Graph converted to SavedModel!')\n",
        "\n",
        "#tf.compat.v1.enable_eager_execution()\n",
        "input_name=\"input\"  #here \n",
        "outputs = ['add_1:0'] #here\n",
        "shutil.rmtree('./saved_model', ignore_errors=True)\n",
        "convert_graph_def_to_saved_model('/root/White-box-Cartoonization/test_code/export/saved_model', '/root/White-box-Cartoonization/test_code/export/white_box_cartoonization_freeze_graph.pb', input_name, outputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE3ofGRsF-Uh"
      },
      "source": [
        "#重啟環境 解決tensorflow版本問題\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dUga7-Dd7Z0"
      },
      "source": [
        "!pip uninstall tensorflow -y\n",
        "!pip install tensorflow==2.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlNfFRNIFfwh"
      },
      "source": [
        "# 4.4 Tensorflow Lite Converter\n",
        "# Error : 無法順利轉換\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "# Weight Quantization - Input/Output=float32\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('/root/White-box-Cartoonization/test_code/export/saved_model')\n",
        "\n",
        "#converter.experimental_new_converter = True   #<--- Not necessary if you are using Tensorflow v2.2.x or later.\n",
        "#converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "tflite_quant_model = converter.convert()\n",
        "#with open('/root/White-box-Cartoonization/test_code/export/saved_model/white_box_cartoonization_weight_quant.tflite', 'wb') as w:\n",
        "#    w.write(tflite_quant_model)\n",
        "#print(\"Weight Quantization complete! - white_box_cartoonization_weight_quant.tflite\")\n",
        "\n",
        "#整數的話有配合 tfds.load \n",
        "#文章後續有介紹 整數量化 / 全整數量化 / 半浮點量化用法\n",
        "#若要使用 tpu 請使用 edgetpu_compiler 再次生成 **_tpu.tflite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XYFETu20EgP"
      },
      "source": [
        "###**(5) ReBuild-Inupt Savemodel to TensorFlow Lite**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF5oJQMRTV_P"
      },
      "source": [
        "#(尚未實現), Tensorflow JS 將會介紹\n",
        "import tensorflow as tf\n",
        "from tensorflow.tools.graph_transforms import TransformGraph\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "\n",
        "    # shape=[1, ?, ?, 3] -> shape=[1, 513, 513, 3]\n",
        "    # name='image' specifies the placeholder name of the converted model\n",
        "    inputs = tf.compat.v1.placeholder(tf.float32, shape=[1, 513, 513, 3], name='image')\n",
        "    with tf.io.gfile.GFile('./model-mobilenet_v1_101.pb', 'rb') as f:\n",
        "        graph_def = tf.compat.v1.GraphDef()\n",
        "    graph_def.ParseFromString(f.read())\n",
        "\n",
        "    # 'image:0' specifies the placeholder name of the model before conversion\n",
        "    tf.graph_util.import_graph_def(graph_def, input_map={'image:0': inputs}, name='')\n",
        "    print([n for n in tf.compat.v1.get_default_graph().as_graph_def().node if n.name == 'image'])\n",
        "\n",
        "    # Delete Placeholder \"image\" before conversion\n",
        "    # see: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms\n",
        "    # TransformGraph(\n",
        "    #     graph_def(),\n",
        "    #     input_op_name,\n",
        "    #     output_op_names,\n",
        "    #     conversion options\n",
        "    # )\n",
        "    optimized_graph_def = TransformGraph(\n",
        "                              tf.compat.v1.get_default_graph().as_graph_def(),\n",
        "                              'image',\n",
        "                              ['heatmap','offset_2','displacement_fwd_2','displacement_bwd_2'],\n",
        "                              ['strip_unused_nodes(type=float, shape=\"1,513,513,3\")'])\n",
        "\n",
        "    tf.io.write_graph(optimized_graph_def, './', 'model-mobilenet_v1_101_513.pb', as_text=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZXV2EIJMHuG"
      },
      "source": [
        "## **[2.Tensorflow Hub 應用方式](https://colab.research.google.com/github/frogermcs/TFLite-Tester/blob/master/notebooks/Testing_TFLite_model.ipynb#scrollTo=9yM9UeUnF8t7)**\n",
        "\n",
        "*   (1) Example : 以範例 Faster RCNN (object detection) 介紹\n",
        "\n",
        "    1.1 載入 TF Hub 模組\n",
        "\n",
        "    1.2 利用 Concrete Func 進行轉換為 tflite"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(1) Example : 以範例 Faster RCNN 介紹**"
      ],
      "metadata": {
        "id": "ouzriiYXuQv8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN7Qt8IfMSr6"
      },
      "source": [
        "!pip install tensorflow_hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFhloNMGMj3b"
      },
      "source": [
        "# faster rcnn\n",
        "import tensorflow_hub as hub\n",
        "hub_model = hub.load('https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1')\n",
        "\n",
        "import tensorflow as tf\n",
        "run_model = tf.function(lambda x : hub_model(x))\n",
        "concrete_func = run_model.get_concrete_function(tf.TensorSpec(shape=[1,None,None,3], dtype=tf.uint8))\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('/root/faster_rcnn_v2_640.tflite', 'wb') as w:\n",
        "    w.write(tflite_quant_model)\n",
        "print(\"Integer Quantization complete! - weights_integer_quant.tflite\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un4d8laJU3Xa"
      },
      "source": [
        "# Load the model.\n",
        "model = hub.load('https://tfhub.dev/google/yamnet/1')\n",
        "\n",
        "# Tensorlfow Hub to TFLite\n",
        "run_model = tf.function(lambda x : model(x))\n",
        "concrete_func = run_model.get_concrete_function(tf.TensorSpec(shape=[None], dtype=tf.float32))\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('/root/yamnet.tflite', 'wb') as w:\n",
        "    w.write(tflite_quant_model)\n",
        "print(\"Integer Quantization complete! - weights_integer_quant.tflite\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKPWpGYUM_Nv"
      },
      "source": [
        "## **3.Tensorflow keras 1.x / 2.x 應用方式**\n",
        "\n",
        "\n",
        "*   *(1) Keras 2.x to TensorFlow Lite*\n",
        "\n",
        "*   *(2) Keras 2.x to TensorFlow Lite*\n",
        "\n",
        "*   *(3) Exaple : 以範例 Faster Grad CAM 介紹*\n",
        "\n",
        "      - 載入 json 架構 與 weight.h5 轉成 tflite (i.MX8MP平台可使用)\n",
        "\n",
        "*   *(4) Extra : ReBuild-Inupt Keras to TensorFlow Lite*\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(1) Keras 1.x to TensorFlow Lite**"
      ],
      "metadata": {
        "id": "LTeOENcxrBBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file( 'model.h5' )\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "with tf.io.gfile.GFile( \"model.tflite\" , 'wb') as f:\n",
        "   f.write(tflite_model)\n",
        "print(\"Quantization complete! - model.tflite \")\n"
      ],
      "metadata": {
        "id": "isTYWn-urHhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(2) Keras 2.x to TensorFlow Lite**"
      ],
      "metadata": {
        "id": "Rvaj4CIJqgkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "model = tf.keras.models.model_from_json(open( \"model.json\" ).read()) \n",
        "model.load_weights('weights.h5')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "with tf.io.gfile.GFile( \"model.tflite\" , 'wb') as f:\n",
        "   f.write(tflite_model)\n",
        "print(\"Quantization complete! - model.tflite \")"
      ],
      "metadata": {
        "id": "3WRzkNFkq9W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(3) Exaple : 以範例 Faster Grad CAM 介紹**"
      ],
      "metadata": {
        "id": "6Ve0xRluqj3u"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJBpS7sFFsRY"
      },
      "source": [
        "%cd /root\n",
        "!git clone https://github.com/shinmura0/Faster-Grad-CAM.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaz8c6d-Nmi0"
      },
      "source": [
        "# TF Lite 量化(一般)\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "model = tf.keras.models.model_from_json(open('/root/Faster-Grad-CAM/model/model.json').read())\n",
        "model.load_weights('/root/Faster-Grad-CAM/model/weights.h5')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('/root/Faster-Grad-CAM/model/weights_weight_quant.tflite', 'wb') as w:\n",
        "    w.write(tflite_quant_model)\n",
        "print(\"Weight Quantization complete! - weights_weight_quant.tflite\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zeY_OPPOcCo"
      },
      "source": [
        "# TF Lite 量化(全整數-生產數據集)\n",
        "%cd /root/Faster-Grad-CAM\n",
        "!git clone https://github.com/karaage0703/janken_dataset.git\n",
        "\n",
        "# 進入該資料夾\n",
        "%cd /root/Faster-Grad-CAM/janken_dataset/gu\n",
        "\n",
        "# 生成 dataset\n",
        "from PIL import Image\n",
        "import os, glob\n",
        "import numpy as np\n",
        "dataset = []\n",
        "files = glob.glob(\"*.JPG\")\n",
        "for file in files:\n",
        "    image = Image.open(file)\n",
        "    image = image.convert(\"RGB\")\n",
        "    data = np.asarray(image)\n",
        "    dataset.append(data)\n",
        "\n",
        "dataset = np.array(dataset)\n",
        "np.save(\"janken_dataset\", dataset) #gererate **.npy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zchmtGkUPVjp",
        "outputId": "0b5705d9-f0e6-4752-bd6a-429fb4697bf0"
      },
      "source": [
        "# TF Lite 量化(全整數)\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def representative_dataset_gen():\n",
        "    raw_test_data = np.load('/root/Faster-Grad-CAM/janken_dataset/gu/janken_dataset.npy')\n",
        "    for image in raw_test_data:\n",
        "        image = tf.image.resize(image, (96, 96))\n",
        "        image = image / 255\n",
        "        calibration_data = image[np.newaxis, :, :, :]\n",
        "        yield [calibration_data]\n",
        "\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "# Integer Quantization - Input/Output=float32\n",
        "# INPUT  = input_1 (float32, 1 x 96 x 96 x 3)\n",
        "# OUTPUT = block_16_expand_relu, global_average_pooling2d_1\n",
        "\n",
        "model = tf.keras.models.model_from_json(open('/root/Faster-Grad-CAM/model/model.json').read())\n",
        "model.load_weights('/root/Faster-Grad-CAM/model/weights.h5')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('/root/Faster-Grad-CAM/model/weights_full_integer_quant.tflite', 'wb') as w:\n",
        "    w.write(tflite_quant_model)\n",
        "print(\"Integer Quantization complete! - weights_integer_quant.tflite\")\n",
        "\n",
        "#文章後續有介紹 整數量化 / 全整數量化 / 半浮點量化用法\n",
        "#若要使用 tpu 請使用 edgetpu_compiler 再次生成 **_tpu.tflite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpytkrb_3o/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpytkrb_3o/assets\n",
            "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Integer Quantization complete! - weights_integer_quant.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T40faxXQeue"
      },
      "source": [
        "### **(4) Extra : ReBuild-Inupt Keras to TensorFlow Lite**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90lCQaAow-nL"
      },
      "source": [
        "\n",
        "# Method  : rebuild and reload weight -> set new input -> save\n",
        "from keras.layers import Input, Flatten, Dense, Dropout, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "input = Input(shape=(480, 720, 3), name='image_input')\n",
        "\n",
        "#re-model\n",
        "initial_model = VGG16(weights='imagenet', include_top=False)\n",
        "for layer in initial_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = Flatten()(initial_model(input))\n",
        "x = Dense(1000, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1000, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(14, activation='linear')(x)\n",
        "\n",
        "model = Model(inputs=input, outputs=x)\n",
        "model.load_weights('my_model_name.h5')\n",
        "\n",
        "\"\"\"\n",
        "inputs2 = Input((512, 512, 3))\n",
        "...\n",
        "x_new\n",
        "model2 = Model(inputs=[inputs2], outputs=[x_new])\n",
        "model2.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model2.set_weights(model.get_weights()) #set old model weitght\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhXjOEz6RGbP"
      },
      "source": [
        "## **4.Tensorflow Java Script 應用方式**\n",
        "\n",
        "*   (1) Java Script to TensorFlow Lite\n",
        "\n",
        "*   (2) Example : 以範例 PoseNet 介紹\n",
        "\n",
        "    2.1 安裝 tfjs-graph-converter\n",
        "\n",
        "    2.2 介紹 savemodel reinput 用法 (尚未試成功)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(1) Java Script to TensorFlow Lite**"
      ],
      "metadata": {
        "id": "F66g4yk_tUDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Json to savedmodel\n",
        "import tfjs_graph_converter.api as tfjs\n",
        "tfjs.graph_model_to_saved_model( \"model.json\" , \"realsavedmodel\" )"
      ],
      "metadata": {
        "id": "vlYkf5z-tyGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorFlow Lite Converter\n",
        "import tensorflow as tf\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model( \"realsavedmodel\" ) \n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "with tf.io.gfile.GFile( \"model.tflite\" , 'wb') as f:\n",
        "   f.write(tflite_model)\n",
        "print(\"Quantization complete! - model.tflite \")"
      ],
      "metadata": {
        "id": "IqReFbqYt03x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(2) Example : 以範例 PoseNet 介紹**"
      ],
      "metadata": {
        "id": "VQBWve4ztPC4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Z5Zx-4ACoj"
      },
      "source": [
        "!pip install tfjs-graph-converter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pojtqnBecaDG"
      },
      "source": [
        "#下載 posenet API\n",
        "%cd /root\n",
        "!git clone https://github.com/atomicbits/posenet-python.git\n",
        "\n",
        "#下載測試圖檔\n",
        "%cd /root/posenet-python\n",
        "!mkdir images\n",
        "%cd images\n",
        "!wget https://i2.kknews.cc/SIG=l6lv5l/46sn0003ss48o8769p27.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3iuVfp1TW-4"
      },
      "source": [
        "# 額外方法 weilly\n",
        "%cd /root/posenet-python\n",
        "!mkdir model\n",
        "%cd model\n",
        "!wget https://storage.googleapis.com/tfjs-models/savedmodel/posenet/mobilenet/quant1/075/model-stride8.json\n",
        "!wget https://storage.googleapis.com/tfjs-models/savedmodel/posenet/mobilenet/quant1/075/model-stride16.json\n",
        "!wget https://storage.googleapis.com/tfjs-models/savedmodel/posenet/mobilenet/quant1/075/group1-shard1of1.bin\n",
        "\n",
        "import tfjs_graph_converter.api as tfjs\n",
        "tfjs.graph_model_to_saved_model(\"/root/posenet-python/model/model-stride16.json\",\"mobilenet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4obfcq_b7U-"
      },
      "source": [
        "%cd /root/posenet-python\n",
        "!python3 image_demo.py --model resnet50 --stride 16 --image_dir ./images --output_dir ./output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd-l9ayiYyBe"
      },
      "source": [
        "#重啟環境 解決tensorflow版本問題\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3Ce2sauYzIs",
        "outputId": "ac4f71c3-8f5c-4fdf-b970-278808e16c45"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XORUDkqoUawQ"
      },
      "source": [
        "### re-input and re-saved model\n",
        "# Error : ValueError: NodeDef mentions attr 'explicit_paddings' not in Op<\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow.tools.graph_transforms import TransformGraph\n",
        "from tensorflow.python.platform import gfile\n",
        "from tensorflow.core.protobuf import saved_model_pb2\n",
        "from tensorflow.python.util import compat\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "\n",
        "    # shape=[1, ?, ?, 3] -> shape=[1, 513, 513, 3]\n",
        "    # name='image' specifies the placeholder name of the converted model\n",
        "\n",
        "    inputs = tf.compat.v1.placeholder(tf.float32, shape=[1, 513, 513, 3], name='image')\n",
        "    #inputs = tf.compat.v1.placeholder(tf.float32, shape=[1, 385, 385, 3], name='image')\n",
        "    #inputs = tf.compat.v1.placeholder(tf.float32, shape=[1, 321, 321, 3], name='image')\n",
        "    #inputs = tf.compat.v1.placeholder(tf.float32, shape=[1, 257, 257, 3], name='image')\n",
        "    #inputs = tf.compat.v1.placeholder(tf.float32, shape=[1, 225, 225, 3], name='image')\n",
        "\n",
        "    with gfile.FastGFile('/root/posenet-python/_tf_models/posenet/resnet50_float/stride16/saved_model.pb', 'rb') as f:\n",
        "        data = compat.as_bytes(f.read())\n",
        "        sm = saved_model_pb2.SavedModel()\n",
        "        sm.ParseFromString(data)\n",
        "        if 1 != len(sm.meta_graphs):\n",
        "            print('More than one graph found. Not sure which to write')\n",
        "            sys.exit(1)\n",
        "\n",
        "    # 'image:0' specifies the placeholder name of the model before conversion\n",
        "    tf.graph_util.import_graph_def(sm.meta_graphs[0].graph_def, input_map={'sub_2:0': inputs}, name='')\n",
        "    print([n for n in tf.compat.v1.get_default_graph().as_graph_def().node if n.name == 'image'])\n",
        "\n",
        "    # Delete Placeholder \"image\" before conversion\n",
        "    # see: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms\n",
        "    # TransformGraph(\n",
        "    #     graph_def(),\n",
        "    #     input_name,\n",
        "    #     output_names,\n",
        "    #     conversion options\n",
        "    # )\n",
        "    optimized_graph_def = TransformGraph(tf.compat.v1.get_default_graph().as_graph_def(),\n",
        "                        'image',\n",
        "                        ['float_heatmaps','float_short_offsets','resnet_v1_50/displacement_fwd_2/BiasAdd','resnet_v1_50/displacement_bwd_2/BiasAdd'],\n",
        "                         ['strip_unused_nodes(type=float, shape=\"1,513,513,3\")'])\n",
        "\n",
        "    tf.io.write_graph(optimized_graph_def, './', 'posenet_resnet50_32_513.pb', as_text=False)\n",
        "\n",
        "#文章後續有介紹 整數量化 / 全整數量化 / 半浮點量化用法\n",
        "#若要使用 tpu 請使用 edgetpu_compiler 再次生成 **_tpu.tflite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVMuEoxsYZP1"
      },
      "source": [
        "### re-input and re-saved model (Some_changes_from_TFv2.x_to_TFv1.x_to_import_into_saved_model)\n",
        "# Error : ValueError: NodeDef mentions attr 'explicit_paddings' not in Op<name=MaxPool\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.platform import gfile\n",
        "from tensorflow.core.protobuf import saved_model_pb2\n",
        "from tensorflow.python.util import compat\n",
        "from tensorflow.tools.graph_transforms import TransformGraph\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "\n",
        "    # shape=[1, ?, ?, 3] -> shape=[1, 513, 513, 3]\n",
        "    # name='image' specifies the placeholder name of the converted model\n",
        "\n",
        "    inputs = tf.compat.v1.placeholder(tf.float32, shape=[1, 224, 224, 3], name='image')\n",
        "    #inputs = tf.compat.v1.placeholder(tf.float32, shape=[1, 385, 385, 3], name='image')\n",
        "    #inputs = tf.compat.v1.placeholder(tf.float32, shape=[1, 321, 321, 3], name='image')\n",
        "    #inputs = tf.compat.v1.placeholder(tf.float32, shape=[1, 257, 257, 3], name='image')\n",
        "    #inputs = tf.compat.v1.placeholder(tf.float32, shape=[1, 225, 225, 3], name='image')\n",
        "\n",
        "    #Some_changes_from_TFv2.x_to_TFv1.x_to_import_into_saved_model\n",
        "    with gfile.FastGFile('/root/posenet-python/model/mobilenet/saved_model.pb', 'rb') as f:\n",
        "      data = compat.as_bytes(f.read())\n",
        "      sm = saved_model_pb2.SavedModel()\n",
        "      sm.ParseFromString(data)\n",
        "      if 1 != len(sm.meta_graphs):\n",
        "        print('More than one graph found. Not sure which to write')\n",
        "        sys.exit(1)\n",
        "\n",
        "    # 'image:0' specifies the placeholder name of the model before conversion\n",
        "    tf.graph_util.import_graph_def(sm.meta_graphs[0].graph_def, input_map={'sub_2:0': inputs}, name='')\n",
        "    print([n for n in tf.compat.v1.get_default_graph().as_graph_def().node if n.name == 'image'])\n",
        "\n",
        "    # Delete Placeholder \"image\" before conversion\n",
        "    # see: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms\n",
        "    # TransformGraph(\n",
        "    #     graph_def(),\n",
        "    #     input_name,\n",
        "    #     output_names,\n",
        "    #     conversion options\n",
        "    # )\n",
        "    optimized_graph_def = TransformGraph(tf.compat.v1.get_default_graph().as_graph_def(),\n",
        "                        'image',\n",
        "                        ['float_heatmaps','float_short_offsets','resnet_v1_50/displacement_fwd_2/BiasAdd','resnet_v1_50/displacement_bwd_2/BiasAdd'],\n",
        "                         ['strip_unused_nodes(type=float, shape=\"1,224,224,3\")'])\n",
        "\n",
        "    tf.io.write_graph(optimized_graph_def, './', 'posenet_mobilnet_224.pb', as_text=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy_tDtGpaO5K"
      },
      "source": [
        "## **5.Tensorflow Slim 應用方式**\n",
        "**量化包含 Tensorflow Lite 不支持但 Tensorflow 支持的操作的模型 (能使用)**\n",
        "*   (1) Example : 以範例 Mask R-CNN for Object Detection and Segmentation 介紹\n",
        "      \n",
        "      1.1 Flex Delegate 試用 \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(1) Example : 以範例 Mask R-CNN 介紹**"
      ],
      "metadata": {
        "id": "ZUL-1oxJusrD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpgMm34UlUwv"
      },
      "source": [
        "#重啟環境 解決tensorflow版本問題\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkG_GVk4WaLC"
      },
      "source": [
        "# 下載 tfslim / ensorflowLite-flexdelegate / Mask_RCNN\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "%cd /root\n",
        "!git clone https://github.com/tensorflow/models.git \n",
        "!git clone https://github.com/matterport/Mask_RCNN/\n",
        "!git clone https://github.com/PINTO0309/TensorflowLite-flexdelegate\n",
        "!wget http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz\n",
        "!tar -zxvf mask_rcnn_inception_v2_coco_2018_01_28.tar.gz\n",
        "!rm mask_rcnn_inception_v2_coco_2018_01_28.tar.gz\n",
        "!mkdir -p /root/mask_rcnn_inception_v2_coco_2018_01_28/export #生成檔案"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQBhtSarhJhD"
      },
      "source": [
        "#安裝與下載 tf slim\n",
        "%cd /root\n",
        "#git clone https://github.com/tensorflow/models.git \n",
        "\n",
        "%cd /root/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=. # gernate *.proto\n",
        "!python setup.py build\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/root/models/research/:/root/models/research/slim/:/root/models/research/object_detection/utils/:/root/models/research/object_detection'\n",
        "!pip install tf_slim\n",
        "!python object_detection/builders/model_builder_test.py\n",
        "\n",
        "!export PYTHONPATH=`pwd`:`pwd`/slim:$PYTHONPATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HyjT9dZhlnd"
      },
      "source": [
        "#生成 freezon graph\n",
        "!python3 /root/models/research/object_detection/export_inference_graph.py \\\n",
        "  --input_type=image_tensor \\\n",
        "  --pipeline_config_path=/root/mask_rcnn_inception_v2_coco_2018_01_28/pipeline.config \\\n",
        "  --trained_checkpoint_prefix=/root/mask_rcnn_inception_v2_coco_2018_01_28/model.ckpt \\\n",
        "  --output_directory=/root/mask_rcnn_inception_v2_coco_2018_01_28/test \\\n",
        "  --input_shape=1,256,256,3 \\\n",
        "  --write_inference_graph=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poNDvxfci3qS"
      },
      "source": [
        "!python3 object_detection/export_inference_graph.py \\\n",
        "  --input_type=image_tensor \\\n",
        "  --pipeline_config_path=/root/mask_rcnn_inception_v2_coco_2018_01_28/pipeline.config \\\n",
        "  --trained_checkpoint_prefix=/root/mask_rcnn_inception_v2_coco_2018_01_28/model.ckpt \\\n",
        "  --output_directory=/root/mask_rcnn_inception_v2_coco_2018_01_28/test \\\n",
        "  --input_shape=1,512,512,3 \\\n",
        "  --write_inference_graph=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1fLhPbalaQp"
      },
      "source": [
        "#重啟環境 解決tensorflow版本問題\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH_DXLvGjU4P",
        "outputId": "1e662969-8969-4999-9204-0e7e290fca31"
      },
      "source": [
        "# Tensorflow Lite 轉換\n",
        "# tensorflow==2.2.0 (該範例無法整數)\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Weight Quantization - Input/Output=float32\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('/root/mask_rcnn_inception_v2_coco_2018_01_28/test/saved_model')\n",
        "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('/root/mask_rcnn_inception_v2_coco_2018_01_28/test/mask_rcnn_inception_v2_coco_weight_quant.tflite', 'wb') as w:\n",
        "    w.write(tflite_quant_model)\n",
        "print(\"Weight Quantization complete! - mask_rcnn_inception_v2_coco_weight_quant.tflite\")\n",
        "#文章後續有介紹 整數量化 / 全整數量化 / 半浮點量化用法\n",
        "#若要使用 tpu 請使用 edgetpu_compiler 再次生成 **_tpu.tflite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "Weight Quantization complete! - mask_rcnn_inception_v2_coco_weight_quant.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GbGvQTZlYMN"
      },
      "source": [
        "## **6.ONNX/OpenVINO Model 應用**\n",
        "\n",
        "\n",
        "*   (1) ONNX to TensorFlow Lite\n",
        "\n",
        "*   (2) [Example : 以 3D Multi-Person Pose Estimation 介紹](https://zhuanlan.zhihu.com/p/370398974)\n",
        "\n",
        "    2.1 安裝 openvino model and tool , onnx2keras\n",
        "\n",
        "    2.2 從 OpenVINO 下載 onnx 模組的方法\n",
        "\n",
        "    2.3 轉換 3D Multi-Person Pose TF Lite 範例\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**(1) ONNX to TensorFlow Lite**"
      ],
      "metadata": {
        "id": "J8bJ8NRSvoNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# onnx-tf install\n",
        "! pip install onnx-tf"
      ],
      "metadata": {
        "id": "bhvOIZe_voc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# onnx to tensorflow\n",
        "! onnx-tf convert -i /root/facemesh.onnx -o /root/facemesh"
      ],
      "metadata": {
        "id": "U5G0N-muvuWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model( \"savedmodel\" )\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "with tf.io.gfile.GFile( \"model.tflite” , 'wb') as f:\n",
        "   f.write(tflite_model)\n",
        "print(\"Quantization complete! - model.tflite \")"
      ],
      "metadata": {
        "id": "Q1ShIi2Ov2WE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**(2) Example : 以 3D Multi-Person Pose Estimation 介紹**"
      ],
      "metadata": {
        "id": "c_U7LfGtv5X7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWc4sONmnBn0"
      },
      "source": [
        "#下載 API\n",
        "#https://docs.openvinotoolkit.org/2020.2/_tools_downloader_README.html\n",
        "%cd /root\n",
        "!git clone https://github.com/opencv/open_model_zoo.git #OpenVINO™ Toolkit - Open Model Zoo repository\n",
        "\n",
        "#安裝 openvino dev tool\n",
        "#https://github.com/openvinotoolkit/openvino\n",
        "!pip install openvino-dev\n",
        "!git clone https://github.com/openvinotoolkit/openvino\n",
        "\n",
        "#安裝 ONNX 2 Keras\n",
        "!pip install onnx2keras\n",
        "\n",
        "#安裝 ONNX 2 tensorflow (weilly建議)\n",
        "%cd /root\n",
        "!git clone https://github.com/onnx/onnx-tensorflow.git \n",
        "%cd onnx-tensorflow\n",
        "!pip install -e .\n",
        "\n",
        "# 下載模組必要套件\n",
        "%cd /root/open_model_zoo/tools/downloader\n",
        "!python -mpip install --user -r ./requirements.in\n",
        "!python -mpip install --user -r ./requirements-tensorflow.in\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_91NTdVpJSC"
      },
      "source": [
        "# 下載模組\n",
        "%cd /root/open_model_zoo/tools/downloader\n",
        "!python downloader.py --name human-pose-estimation-3d-0001 --precisions FP32\n",
        "!./converter.py --mo /root/openvino_env/openvino/model-optimizer/mo.py --name human-pose-estimation-3d-0001\n",
        "#here -> /root/open_model_zoo/tools/downloader/public/human-pose-estimation-3d-0001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOpcSIQRm4_R"
      },
      "source": [
        "# onnx 轉成 saveedmodel -方法1\n",
        "import onnx\n",
        "from onnx2keras import onnx_to_keras\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "\n",
        "onnx_model = onnx.load('/root/open_model_zoo/tools/downloader/public/human-pose-estimation-3d-0001/human-pose-estimation-3d-0001.onnx')\n",
        "k_model = onnx_to_keras(onnx_model=onnx_model, input_names=['data'], change_ordering=True)\n",
        "\n",
        "shutil.rmtree('/root/open_model_zoo/tools/downloader/public/human-pose-estimation-3d-0001/model/saved_model', ignore_errors=True)\n",
        "tf.saved_model.save(k_model, '/root/open_model_zoo/tools/downloader/public/human-pose-estimation-3d-0001/model/saved_model')\n",
        "\n",
        "# onnx 轉成 saveedmodel -方法2\n",
        "#!onnx-tf convert -i /root/open_model_zoo/tools/downloader/public/human-pose-estimation-3d-0001/human-pose-estimation-3d-0001.onnx -o /root/open_model_zoo/tools/downloader/public/human-pose-estimation-3d-0001/model/saved_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXe9slRBsv7u"
      },
      "source": [
        "#重啟環境 解決tensorflow版本問題\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNeNjG526Bss"
      },
      "source": [
        "!pip install tensorflow==2.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc-kedZToJ7v",
        "outputId": "190e5f44-2f7d-4135-fc2f-02683ee50946"
      },
      "source": [
        "# Tensorflow Lite 轉換\n",
        "# Tensorflow 2.1.0 =>  Aborted (core dumped)\n",
        "# Tensorflow 2.2.0 => Run => resolved reporter, Segmentation fault\n",
        "# Tensorflow 2.5.0 => Run => didn't find op for builtin opcode \"CONV\" version 5\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "def representative_dataset_gen():\n",
        "    for _ in range(50):\n",
        "        yield [np.random.uniform(0.0, 1.0, size=(1, 3, 256, 448)).astype(np.float32)] #需要輸入端大小\n",
        "\n",
        "# Weight Quantization - Input/Output=float32\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('/root/open_model_zoo/tools/downloader/public/human-pose-estimation-3d-0001/model/saved_model')\n",
        "#converter = tf.compat.v1.lite.TFLiteConverter.from_saved_model('/root/open_model_zoo/tools/downloader/public/human-pose-estimation-3d-0001/model/saved_model')\n",
        "#converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "#converter.inference_input_type  = tf.uint8\n",
        "#converter.inference_output_type = tf.uint8\n",
        "#converter.representative_dataset = representative_dataset_gen\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('/root/open_model_zoo/tools/downloader/public/human-pose-estimation-3d-0001/model/human_pose_estimation_3d_0001_256x448_weight_quant.tflite', 'wb') as w:\n",
        "    w.write(tflite_quant_model)\n",
        "print(\"Weight Quantization complete! - human_pose_estimation_3d_0001_256x448_weight_quant.tflite\")\n",
        "#文章後續有介紹 整數量化 / 全整數量化 / 半浮點量化用法\n",
        "#若要使用 tpu 請使用 edgetpu_compiler 再次生成 **_tpu.tflite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weight Quantization complete! - human_pose_estimation_3d_0001_256x448_weight_quant.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "m-BihhEWwbYu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv7-kbsgsPPg"
      },
      "source": [
        "## 7.**MediaPipe 應用方式 (TensorFlow Lite Inverter)**\n",
        "\n",
        "[@PINTO 大神](https://zenn.dev/pinto0309/articles/9d316860f8d418)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(1) Example : 以 BlazeFace(.tflite) 介紹**"
      ],
      "metadata": {
        "id": "1S-qgF-MwYoL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGqRlJ98bRkR",
        "outputId": "38ff3a42-e375-44bc-fce5-825222ef179c"
      },
      "source": [
        "# 還原模組跟版本有很大關係\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCXp5YbxbMPf"
      },
      "source": [
        "#重啟環境 解決tensorflow版本問題\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGYaH99FzCFk"
      },
      "source": [
        "#重新安裝 Tensorflow\n",
        "!pip uninstall tensorflow -y\n",
        "!pip install tensorflow==2.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WKC3dyj8xdc"
      },
      "source": [
        "#確認 Tensorflow 版本\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gjnZaBkogEc"
      },
      "source": [
        "# 安裝 Flat Buffers (FlatBuffers是一個高效的跨平台序列化庫)\n",
        "%cd /root\n",
        "!git clone https://github.com/google/flatbuffers.git # flatbuffers ? what??\n",
        "\n",
        "%cd /root/flatbuffers\n",
        "#!git checkout v1.11.0\n",
        "\n",
        "%cd /root/flatbuffers\n",
        "!cmake -G \"Unix Makefiles\"\n",
        "!make"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE7YiA_zxoI-"
      },
      "source": [
        "# Download Model\n",
        "#!link google driver\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# copy openvino file\n",
        "!cp /content/gdrive/MyDrive/\"Colab DataSet\"/\"OpenVINO Model\"/schema_TF1x15_5.fbs /root/flatbuffers/schema.fbs\n",
        "!cp /content/gdrive/MyDrive/\"Colab DataSet\"/\"OpenVINO Model\"/BlazeFace/face_detection_front.tflite /root/flatbuffers/face_detection_front.tflite\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7s3pHlpyguB"
      },
      "source": [
        "# API\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# API config\n",
        "%cd /root/flatbuffers/\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "schema = \"schema.fbs\"\n",
        "binary = \"./flatc\"\n",
        "model_path = \"palm_detection.tflite\"\n",
        "output_pb_path = \"palm_detection.pb\"\n",
        "output_savedmodel_path = \"saved_model\"\n",
        "model_json_path = \"palm_detection.json\"\n",
        "num_tensors = 256 #須查看 Netron => Output's location +1\n",
        "output_node_names = ['classificators', 'regressors']\n",
        "\n",
        "# API\n",
        "# 產生架構檔(JSON)\n",
        "def gen_model_json():\n",
        "  cmd = (binary + \" -t --strict-json --defaults-json -o . {schema} -- {input}\".format(input=model_path, schema=schema))\n",
        "  print(\"output json command =\", cmd)\n",
        "  os.system(cmd)\n",
        "\n",
        "# 解析架構\n",
        "def parse_json():\n",
        "    j = json.load(open(model_json_path))\n",
        "    op_types = [v['builtin_code'] for v in j['operator_codes']]\n",
        "    #print('op types:', op_types)\n",
        "    ops = j['subgraphs'][0]['operators'] # subgraphs => 分為 tensors / inputs / outputs / operators\n",
        "    # print('num of ops:', len(ops))\n",
        "    return ops, op_types\n",
        "\n",
        "\n",
        "def make_graph(ops, op_types, interpreter):\n",
        "    tensors = {}\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    # print(input_details)\n",
        "    for input_detail in input_details:\n",
        "        tensors[input_detail['index']] = tf.compat.v1.placeholder(\n",
        "            dtype=input_detail['dtype'],\n",
        "            shape=input_detail['shape'],\n",
        "            name=input_detail['name'])\n",
        "\n",
        "    for index, op in enumerate(ops):\n",
        "        print('op: ', op)\n",
        "        op_type = op_types[op['opcode_index']]\n",
        "        if op_type == 'CONV_2D':\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            weights_detail = interpreter._get_tensor_details(op['inputs'][1])\n",
        "            bias_detail = interpreter._get_tensor_details(op['inputs'][2])\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            # print('weights_detail: ', weights_detail)\n",
        "            # print('bias_detail: ', bias_detail)\n",
        "            # print('output_detail: ', output_detail)\n",
        "            weights_array = interpreter.get_tensor(weights_detail['index'])\n",
        "            weights_array = np.transpose(weights_array, (1, 2, 3, 0))\n",
        "            bias_array = interpreter.get_tensor(bias_detail['index'])\n",
        "            weights = tf.compat.v1.Variable(weights_array, name=weights_detail['name'])\n",
        "            bias = tf.compat.v1.Variable(bias_array, name=bias_detail['name'])\n",
        "            options = op['builtin_options']\n",
        "            output_tensor = tf.compat.v1.nn.conv2d(\n",
        "                input_tensor,\n",
        "                weights,\n",
        "                strides=[1, options['stride_h'], options['stride_w'], 1],\n",
        "                padding=options['padding'],\n",
        "                dilations=[\n",
        "                    1, options['dilation_h_factor'],\n",
        "                    options['dilation_w_factor'], 1\n",
        "                ],\n",
        "                name=output_detail['name'] + '/conv2d')\n",
        "            output_tensor = tf.compat.v1.add(\n",
        "                output_tensor, bias, name=output_detail['name'])\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "        elif op_type == 'DEPTHWISE_CONV_2D':\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            weights_detail = interpreter._get_tensor_details(op['inputs'][1])\n",
        "            bias_detail = interpreter._get_tensor_details(op['inputs'][2])\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            # print('weights_detail: ', weights_detail)\n",
        "            # print('bias_detail: ', bias_detail)\n",
        "            # print('output_detail: ', output_detail)\n",
        "            weights_array = interpreter.get_tensor(weights_detail['index'])\n",
        "            weights_array = np.transpose(weights_array, (1, 2, 3, 0))\n",
        "            bias_array = interpreter.get_tensor(bias_detail['index'])\n",
        "            weights = tf.compat.v1.Variable(weights_array, name=weights_detail['name'])\n",
        "            bias = tf.compat.v1.Variable(bias_array, name=bias_detail['name'])\n",
        "            options = op['builtin_options']\n",
        "            output_tensor = tf.compat.v1.nn.depthwise_conv2d(\n",
        "                input_tensor,\n",
        "                weights,\n",
        "                strides=[1, options['stride_h'], options['stride_w'], 1],\n",
        "                padding=options['padding'],\n",
        "                # dilations=[\n",
        "                #     1, options['dilation_h_factor'],\n",
        "                #     options['dilation_w_factor'], 1\n",
        "                # ],\n",
        "                name=output_detail['name'] + '/depthwise_conv2d')\n",
        "            output_tensor = tf.compat.v1.add(\n",
        "                output_tensor, bias, name=output_detail['name'])\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "        elif op_type == 'MAX_POOL_2D':\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            options = op['builtin_options']\n",
        "            output_tensor = tf.compat.v1.nn.max_pool(\n",
        "                input_tensor,\n",
        "                ksize=[\n",
        "                    1, options['filter_height'], options['filter_width'], 1\n",
        "                ],\n",
        "                strides=[1, options['stride_h'], options['stride_w'], 1],\n",
        "                padding=options['padding'],\n",
        "                name=output_detail['name'])\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "        elif op_type == 'PAD':\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            paddings_detail = interpreter._get_tensor_details(op['inputs'][1])\n",
        "            print('output_detail:', output_detail)\n",
        "            print('paddings_detail:', paddings_detail)\n",
        "            paddings_array = interpreter.get_tensor(paddings_detail['index'])\n",
        "            paddings = tf.compat.v1.Variable(paddings_array, name=paddings_detail['name'])\n",
        "            paddings = tf.constant(paddings_array)\n",
        "            print('paddings:',paddings)\n",
        "            output_tensor = tf.compat.v1.pad(\n",
        "                input_tensor, paddings, name=output_detail['name'])\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "        elif op_type == 'RELU':\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            output_tensor = tf.compat.v1.nn.relu(\n",
        "                input_tensor, name=output_detail['name'])\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "        elif op_type == 'PRELU':\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            alpha_detail = interpreter._get_tensor_details(op['inputs'][1])\n",
        "            alpha_array = interpreter.get_tensor(alpha_detail['index'])\n",
        "            with tf.compat.v1.variable_scope(name_or_scope=output_detail['name']):\n",
        "                alphas = tf.compat.v1.Variable(alpha_array, name=alpha_detail['name'])\n",
        "                output_tensor = tf.compat.v1.maximum(alphas * input_tensor, input_tensor)\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "        elif op_type == 'RESHAPE':\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            options = op['builtin_options']\n",
        "            output_tensor = tf.compat.v1.reshape(\n",
        "                input_tensor, options['new_shape'], name=output_detail['name'])\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "\n",
        "        elif op_type == 'RESIZE_BILINEAR':\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            size_detail = interpreter._get_tensor_details(op['inputs'][1])\n",
        "            size = interpreter.get_tensor(size_detail['index'])\n",
        "            size_height = size[0]\n",
        "            size_width  = size[1]\n",
        "            tensors[output_detail['index']] = tf.image.resize(input_tensor, [size_height, size_width], method='bilinear')\n",
        "            #def upsampling2d_bilinear(x, size_height, size_width):\n",
        "            #  return  tf.image.resize(x, [size_height, size_width], method='bilinear')\n",
        "            #output_tensor = tf.keras.layers.Lambda(upsampling2d_bilinear, arguments={'size_height': size_height, 'size_width': size_width})(input_tensor)\n",
        "            #tensors[output_detail['index']] = output_tensor\n",
        "\n",
        "        elif op_type == 'ADD':\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            input_tensor_0 = tensors[op['inputs'][0]]\n",
        "            input_tensor_1 = tensors[op['inputs'][1]]\n",
        "            output_tensor = tf.compat.v1.add(input_tensor_0, input_tensor_1, name=output_detail['name'])\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "\n",
        "        elif op_type == 'CONCATENATION':\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            input_tensor_0 = tensors[op['inputs'][0]]\n",
        "            input_tensor_1 = tensors[op['inputs'][1]]\n",
        "            options = op['builtin_options']\n",
        "            output_tensor = tf.compat.v1.concat([input_tensor_0, input_tensor_1],\n",
        "                                      options['axis'],\n",
        "                                      name=output_detail['name'])\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "        elif op_type == 'AVERAGE_POOL_2D':\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            options = op['builtin_options']\n",
        "            pool_size = [options['filter_height'], options['filter_width']]\n",
        "            strides = [options['stride_h'], options['stride_w']]\n",
        "            padding = options['padding']\n",
        "            output_tensor = tf.keras.layers.AveragePooling2D(pool_size=pool_size,\n",
        "                                      strides=strides,\n",
        "                                      padding=padding,\n",
        "                                      name=output_detail['name'])(input_tensor)\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "        elif op_type == 'SOFTMAX':\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            output_tensor = tf.compat.v1.nn.softmax(input_tensor, name=output_detail['name'])\n",
        "        elif op_type == 'DEQUANTIZE':\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            weights_detail = interpreter._get_tensor_details(op['inputs'][0])\n",
        "            weights = interpreter.get_tensor(weights_detail['index'])\n",
        "            output_tensor = weights.astype(np.float32)\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "        else:\n",
        "            raise ValueError(op_type)\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# MAIN\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# 將 tf lite 重新拆解回 savedmodel\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "gen_model_json()\n",
        "ops, op_types = parse_json()\n",
        "interpreter = tf.lite.Interpreter(model_path)\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "print(input_details)\n",
        "print(output_details)\n",
        "\n",
        "make_graph(ops, op_types, interpreter) #error\n",
        "\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "graph = tf.compat.v1.get_default_graph()\n",
        "\n",
        "with tf.compat.v1.Session(config=config, graph=graph) as sess:\n",
        "  sess.run(tf.compat.v1.global_variables_initializer())\n",
        "  graph_def = tf.compat.v1.graph_util.convert_variables_to_constants(sess=sess,input_graph_def=graph.as_graph_def(),output_node_names=output_node_names)\n",
        "\n",
        "  with tf.io.gfile.GFile(output_pb_path, 'wb') as f:\n",
        "    f.write(graph_def.SerializeToString())\n",
        "\n",
        "  shutil.rmtree('saved_model', ignore_errors=True)\n",
        "  tf.compat.v1.saved_model.simple_save(sess,\n",
        "            output_savedmodel_path,\n",
        "            inputs={'input': graph.get_tensor_by_name('input:0')},\n",
        "            outputs={\n",
        "                #'MobilenetV1/Predictions/Reshape_1' : graph.get_tensor_by_name('MobilenetV1/Predictions/Reshape_1:0')\n",
        "                'classificators': graph.get_tensor_by_name('classificators:0'),\n",
        "                'regressors': graph.get_tensor_by_name('regressors:0')\n",
        "            })\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCBXatQHfmQl"
      },
      "source": [
        "# 檢視 op_types 型態狀況\n",
        "# 如果錯誤的話 (都是ADD) 則須重新調用匹配 Tesnorflow 與 schema.fbs 版本\n",
        "\n",
        "#!./flatc -t --strict-json --defaults-json -o . schema.fbs -- mobilenet_v1_1.0_224.tflite\n",
        "ops, op_types = parse_json()\n",
        "print(op_types)\n",
        "\n",
        "# 可檢視之項目 \"version\" \"operator_codes\" \"subgraphs\" \"description\" \"buffers\" \"metadata_buffer\"\n",
        "'''\n",
        "j = json.load(open(model_json_path))\n",
        "j['version']\n",
        "j['operator_codes']\n",
        "j['subgraphs']\n",
        "j['description']\n",
        "j['buffers']\n",
        "j['metadata_buffer']\n",
        "'''\n",
        "\n",
        "# 可以進一步細分 subgraphs 之項目 \"tensors\" \"inputs\" \"outputs\" \"operators\"\n",
        "'''\n",
        "j['subgraphs'][0]['tensors']\n",
        "j['subgraphs'][0]['inputs']\n",
        "j['subgraphs'][0]['outputs']\n",
        "j['subgraphs'][0]['operators']\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK8m5EM8mV2j"
      },
      "source": [
        "# Tensorflow Lite 轉換 (浮點)\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "converter = tf.compat.v1.lite.TFLiteConverter.from_saved_model('/root/flatbuffers/saved_model')\n",
        "#converter.optimizations = [tf.compat.v1.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "#converter.target_spec.supported_ops = [tf.compat.v1.lite.OpsSet.TFLITE_BUILTINS, tf.compat.v1.lite.OpsSet.SELECT_TF_OPS]\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('/root/palm_detection_float_1.tflite', 'wb') as w:\n",
        "    w.write(tflite_quant_model)\n",
        "print(\"Weight Quantization complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUMj0kIjGMjT"
      },
      "source": [
        "# Tensorflow Lite 轉換 (全整數) \n",
        "# 可以使用但會出現 error : undefined identifier: 訊息\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "def representative_dataset_gen():\n",
        "    for _ in range(50):\n",
        "        yield [np.random.uniform(0.0, 1.0, size=(1, 128, 128, 3)).astype(np.float32)] #需要輸入端大小\n",
        "\n",
        "converter = tf.compat.v1.lite.TFLiteConverter.from_saved_model('/root/flatbuffers/saved_model')\n",
        "#converter.optimizations = [tf.compat.v1.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "converter.target_spec.supported_ops = [tf.compat.v1.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type  = tf.compat.v1.uint8\n",
        "converter.inference_output_type = tf.compat.v1.uint8\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('/root/palm_detection_uint8.tflite', 'wb') as w:\n",
        "    w.write(tflite_quant_model)\n",
        "print(\"Weight Quantization complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByvYJzE17aKp"
      },
      "source": [
        "### **(2) Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wNUJgbk7zcH"
      },
      "source": [
        "# 可用 keras 描述\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# API config\n",
        "%cd /root/flatbuffers/\n",
        "!rm -r /root/flatbuffers/saved_model\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "schema = \"schema.fbs\"\n",
        "binary = \"./flatc\"\n",
        "model_path = \"posenet.tflite\"\n",
        "output_pb_path = \"posenet.pb\"\n",
        "output_savedmodel_path = \"saved_model\"\n",
        "model_json_path = \"posenet.json\"\n",
        "output_node_names = ['MobilenetV1/heatmap_2/BiasAdd','MobilenetV1/offset_2/BiasAdd','MobilenetV1/displacement_fwd_2/BiasAdd','MobilenetV1/displacement_bwd_2/BiasAdd']\n",
        "\n",
        "# API\n",
        "# 產生架構檔(JSON)\n",
        "def gen_model_json():\n",
        "  cmd = (binary + \" -t --strict-json --defaults-json -o . {schema} -- {input}\".format(input=model_path, schema=schema))\n",
        "  print(\"output json command =\", cmd)\n",
        "  os.system(cmd)\n",
        "\n",
        "# 解析架構\n",
        "def parse_json():\n",
        "    j = json.load(open(model_json_path))\n",
        "    op_types = [v['builtin_code'] for v in j['operator_codes']]\n",
        "    #print('op types:', op_types)\n",
        "    ops = j['subgraphs'][0]['operators'] # subgraphs => 分為 tensors / inputs / outputs / operators\n",
        "    print('num of ops:', len(ops))\n",
        "    return ops, op_types\n",
        "\n",
        "def make_graph(ops, op_types, interpreter):\n",
        "    tensors = {}\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    # print(input_details)\n",
        "    for input_detail in input_details:\n",
        "        tensors[input_detail['index']] = tf.compat.v1.placeholder(\n",
        "            dtype=input_detail['dtype'],\n",
        "            shape=input_detail['shape'],\n",
        "            name=input_detail['name'])\n",
        "\n",
        "    for index, op in enumerate(ops):\n",
        "        print('op: ', op)\n",
        "        op_type = op_types[op['opcode_index']]\n",
        "        if op_type == 'CONV_2D':\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            weights_detail = interpreter._get_tensor_details(op['inputs'][1])\n",
        "            bias_detail = interpreter._get_tensor_details(op['inputs'][2])\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            # print('weights_detail: ', weights_detail)\n",
        "            # print('bias_detail: ', bias_detail)\n",
        "            # print('output_detail: ', output_detail)\n",
        "            weights_array = interpreter.get_tensor(weights_detail['index'])\n",
        "            weights_array = np.transpose(weights_array, (1, 2, 3, 0))\n",
        "            bias_array = interpreter.get_tensor(bias_detail['index'])\n",
        "            weights = tf.compat.v1.Variable(weights_array, name=weights_detail['name'])\n",
        "            bias = tf.compat.v1.Variable(bias_array, name=bias_detail['name'])\n",
        "            options = op['builtin_options']\n",
        "            output_tensor = tf.compat.v1.nn.conv2d(\n",
        "                input_tensor,\n",
        "                weights,\n",
        "                strides=[1, options['stride_h'], options['stride_w'], 1],\n",
        "                padding=options['padding'],\n",
        "                dilations=[\n",
        "                    1, options['dilation_h_factor'],\n",
        "                    options['dilation_w_factor'], 1\n",
        "                ],\n",
        "                name=output_detail['name'] + '/conv2d')\n",
        "            #output_tensor = tf.compat.v1.add(output_tensor, bias, name=output_detail['name'])\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "        elif op_type == 'DEPTHWISE_CONV_2D':\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            weights_detail = interpreter._get_tensor_details(op['inputs'][1])\n",
        "            bias_detail = interpreter._get_tensor_details(op['inputs'][2])\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            # print('weights_detail: ', weights_detail)\n",
        "            # print('bias_detail: ', bias_detail)\n",
        "            # print('output_detail: ', output_detail)\n",
        "            weights_array = interpreter.get_tensor(weights_detail['index'])\n",
        "            weights_array = np.transpose(weights_array, (1, 2, 3, 0))\n",
        "            bias_array = interpreter.get_tensor(bias_detail['index'])\n",
        "            weights = tf.compat.v1.Variable(weights_array, name=weights_detail['name'])\n",
        "            bias = tf.compat.v1.Variable(bias_array, name=bias_detail['name'])\n",
        "            options = op['builtin_options']\n",
        "            output_tensor = tf.compat.v1.nn.depthwise_conv2d(\n",
        "                input_tensor,\n",
        "                weights,\n",
        "                strides=[1, options['stride_h'], options['stride_w'], 1],\n",
        "                padding=options['padding'],\n",
        "                # dilations=[\n",
        "                #     1, options['dilation_h_factor'],\n",
        "                #     options['dilation_w_factor'], 1\n",
        "                # ],\n",
        "                name=output_detail['name'] + '/depthwise_conv2d')\n",
        "            #output_tensor = tf.compat.v1.add(output_tensor, bias, name=output_detail['name'])\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "        elif op_type == 'MAX_POOL_2D':\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            options = op['builtin_options']\n",
        "            output_tensor = tf.compat.v1.nn.max_pool(\n",
        "                input_tensor,\n",
        "                ksize=[\n",
        "                    1, options['filter_height'], options['filter_width'], 1\n",
        "                ],\n",
        "                strides=[1, options['stride_h'], options['stride_w'], 1],\n",
        "                padding=options['padding'],\n",
        "                name=output_detail['name'])\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "        elif op_type == 'PAD':\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            paddings_detail = interpreter._get_tensor_details(op['inputs'][1])\n",
        "            print('output_detail:', output_detail)\n",
        "            print('paddings_detail:', paddings_detail)\n",
        "            paddings_array = interpreter.get_tensor(paddings_detail['index'])\n",
        "            paddings = tf.compat.v1.Variable(paddings_array, name=paddings_detail['name'])\n",
        "            paddings = tf.constant(paddings_array)\n",
        "            print('paddings:',paddings)\n",
        "            output_tensor = tf.compat.v1.pad(\n",
        "                input_tensor, paddings, name=output_detail['name'])\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "        elif op_type == 'RELU':\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            output_tensor = tf.compat.v1.nn.relu(\n",
        "                input_tensor, name=output_detail['name'])\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "        elif op_type == 'PRELU':\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            alpha_detail = interpreter._get_tensor_details(op['inputs'][1])\n",
        "            alpha_array = interpreter.get_tensor(alpha_detail['index'])\n",
        "            with tf.compat.v1.variable_scope(name_or_scope=output_detail['name']):\n",
        "                alphas = tf.compat.v1.Variable(alpha_array, name=alpha_detail['name'])\n",
        "                output_tensor = tf.compat.v1.maximum(alphas * input_tensor, input_tensor)\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "        elif op_type == 'RESHAPE':\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            options = op['builtin_options']\n",
        "            output_tensor = tf.compat.v1.reshape(\n",
        "                input_tensor, options['new_shape'], name=output_detail['name'])\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "\n",
        "        elif op_type == 'RESIZE_BILINEAR':\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            size_detail = interpreter._get_tensor_details(op['inputs'][1])\n",
        "            size = interpreter.get_tensor(size_detail['index'])\n",
        "            size_height = size[0]\n",
        "            size_width  = size[1]\n",
        "            tensors[output_detail['index']] = tf.image.resize(input_tensor, [size_height, size_width], method='bilinear')\n",
        "            #def upsampling2d_bilinear(x, size_height, size_width):\n",
        "            #  return  tf.image.resize(x, [size_height, size_width], method='bilinear')\n",
        "            #output_tensor = tf.keras.layers.Lambda(upsampling2d_bilinear, arguments={'size_height': size_height, 'size_width': size_width})(input_tensor)\n",
        "            #tensors[output_detail['index']] = output_tensor\n",
        "\n",
        "        elif op_type == 'ADD':\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            input_tensor_0 = tensors[op['inputs'][0]]\n",
        "            input_tensor_1 = tensors[op['inputs'][1]]\n",
        "            output_tensor = tf.compat.v1.add(input_tensor_0, input_tensor_1, name=output_detail['name'])\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "\n",
        "        elif op_type == 'CONCATENATION':\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            input_tensor_0 = tensors[op['inputs'][0]]\n",
        "            input_tensor_1 = tensors[op['inputs'][1]]\n",
        "            options = op['builtin_options']\n",
        "            output_tensor = tf.compat.v1.concat([input_tensor_0, input_tensor_1],\n",
        "                                      options['axis'],\n",
        "                                      name=output_detail['name'])\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "        elif op_type == 'AVERAGE_POOL_2D':\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            options = op['builtin_options']\n",
        "            pool_size = [options['filter_height'], options['filter_width']]\n",
        "            strides = [options['stride_h'], options['stride_w']]\n",
        "            padding = options['padding']\n",
        "            output_tensor = tf.keras.layers.AveragePooling2D(pool_size=pool_size,\n",
        "                                      strides=strides,\n",
        "                                      padding=padding,\n",
        "                                      name=output_detail['name'])(input_tensor)\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "        elif op_type == 'SOFTMAX':\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            input_tensor = tensors[op['inputs'][0]]\n",
        "            output_tensor = tf.compat.v1.nn.softmax(input_tensor, name=output_detail['name'])\n",
        "        elif op_type == 'DEQUANTIZE':\n",
        "            output_detail = interpreter._get_tensor_details(op['outputs'][0])\n",
        "            weights_detail = interpreter._get_tensor_details(op['inputs'][0])\n",
        "            weights = interpreter.get_tensor(weights_detail['index'])\n",
        "            output_tensor = weights.astype(np.float32)\n",
        "            tensors[output_detail['index']] = output_tensor\n",
        "        else:\n",
        "            raise ValueError(op_type)\n",
        "\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# MAIN\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# 將 tf lite 重新拆解回 savedmodel\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "gen_model_json()\n",
        "ops, op_types = parse_json()\n",
        "interpreter = tf.lite.Interpreter(model_path)\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "print(input_details)\n",
        "print(output_details)\n",
        "\n",
        "make_graph(ops, op_types, interpreter) #error\n",
        "\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "graph = tf.compat.v1.get_default_graph()\n",
        "\n",
        "with tf.compat.v1.Session(config=config, graph=graph) as sess:\n",
        "  sess.run(tf.compat.v1.global_variables_initializer())\n",
        "  graph_def = tf.compat.v1.graph_util.convert_variables_to_constants(sess=sess,input_graph_def=graph.as_graph_def(),output_node_names=output_node_names)\n",
        "\n",
        "  with tf.io.gfile.GFile(output_pb_path, 'wb') as f:\n",
        "    f.write(graph_def.SerializeToString())\n",
        "\n",
        "  shutil.rmtree('saved_model', ignore_errors=True)\n",
        "  tf.compat.v1.saved_model.simple_save(sess,\n",
        "            output_savedmodel_path,\n",
        "            inputs={'sub_2': graph.get_tensor_by_name('sub_2:0')},\n",
        "            outputs={\n",
        "                'MobilenetV1/heatmap_2/BiasAdd' : graph.get_tensor_by_name('MobilenetV1/heatmap_2/BiasAdd:0'),\n",
        "                'MobilenetV1/offset_2/BiasAdd' : graph.get_tensor_by_name('MobilenetV1/offset_2/BiasAdd:0'),\n",
        "                'MobilenetV1/displacement_fwd_2/BiasAdd' : graph.get_tensor_by_name('MobilenetV1/displacement_fwd_2/BiasAdd:0'),\n",
        "                'MobilenetV1/displacement_bwd_2/BiasAdd' : graph.get_tensor_by_name('MobilenetV1/displacement_bwd_2/BiasAdd:0')\n",
        "            })\n",
        "  \n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBHuLwZ4-Ldc",
        "outputId": "ec90c1d0-746f-4245-e62f-df29e6d9d457"
      },
      "source": [
        "ops, op_types = parse_json()\n",
        "print(op_types)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['CONV_2D', 'DEPTHWISE_CONV_2D']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URo4c9yX9U1z"
      },
      "source": [
        "# Tensorflow Lite 轉換 (浮點)\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "converter = tf.compat.v1.lite.TFLiteConverter.from_saved_model('/root/flatbuffers/saved_model')\n",
        "#converter.optimizations = [tf.compat.v1.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "#converter.target_spec.supported_ops = [tf.compat.v1.lite.OpsSet.TFLITE_BUILTINS, tf.compat.v1.lite.OpsSet.SELECT_TF_OPS]\n",
        "tflite_quant_model = converter.convert()\n",
        "with open('/root/posenet_new.tflite', 'wb') as w:\n",
        "    w.write(tflite_quant_model)\n",
        "print(\"Weight Quantization complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CidZSHYosP6"
      },
      "source": [
        "**Problem　１　：　tf2 converter**\n",
        "\n",
        "UnliftableError: A SavedModel signature needs an input for each placeholder the signature's outputs use.\n",
        "\n",
        "(solve) : 儲存模組的代碼寫錯 \n",
        "\n",
        "\"---------------------------------------------------------------------------------------------------------------------------------------------\"\n",
        "\n",
        "**Problem　2　：　tf1 converter**\n",
        "\n",
        "ConverterError: \"Graph does not contain node Input.\n",
        "\n",
        "(solve) : 儲存模組的代碼寫錯 \n",
        "\n",
        "\"---------------------------------------------------------------------------------------------------------------------------------------------\"\n",
        "\n",
        "**Problem　3　：　tf lite runtime**\n",
        "\n",
        "ERROR: Didn't find op for builtin opcode 'CONV_2D' version '5'.\n",
        "\n",
        "(solve) : 降低 Tensorflow 版本至 2.1.0, 但衍伸問題4\n",
        "\n",
        "\"---------------------------------------------------------------------------------------------------------------------------------------------\"\n",
        "\n",
        "**Problem　4　：　tf lite runtime**\n",
        "\n",
        "Error : ERROR: Node number 164 (TfLiteNnapiDelegate) failed to invoke.\n",
        "\n",
        "(solve) : num_tensors 數量設錯\n",
        "\n",
        "\"---------------------------------------------------------------------------------------------------------------------------------------------\"\n",
        "\n",
        "**Problem　5　：　tf lite runtime**\n",
        "\n",
        "Error : Netron 架構多了 Dequantize 這層\n",
        "\n",
        "(solve) : 不進行 converter.optimizations 轉換"
      ]
    }
  ]
}